### AI-ML-Practise

### 1. **Supervised Learning:**
   - **Definition:** Learning a function that maps input data to output data based on example input-output pairs.
   - **Algorithms:** Linear Regression, Decision Trees, Support Vector Machines, Neural Networks.

### 2. **Unsupervised Learning:**
   - **Definition:** Extracting patterns or representations from data without labeled outputs.
   - **Types:** Clustering (K-Means, Hierarchical), Dimensionality Reduction (PCA, t-SNE).

### 3. **Cross-Validation:**
   - **Purpose:** Assessing a model's performance on unseen data to detect overfitting.
   - **Techniques:** k-Fold Cross-Validation, Holdout Validation.

### 4. **Bias-Variance Tradeoff:**
   - **Definition:** Balancing errors from underfitting (high bias) and overfitting (high variance).
   - **Optimization:** Adjusting model complexity, regularization.

### 5. **Feature Engineering:**
   - **Definition:** Creating new features from existing ones to improve model performance.
   - **Techniques:** Polynomial features, Interaction terms, Feature scaling.

### 6. **Ensemble Learning:**
   - **Definition:** Combining predictions from multiple models to improve overall performance.
   - **Algorithms:** Random Forest, Gradient Boosting.

### 7. **Hyperparameter Tuning:**
   - **Definition:** Adjusting parameters that are not learned during training.
   - **Methods:** Grid Search, Random Search, Bayesian Optimization.

### 8. **Evaluation Metrics:**
   - **Regression:** MAE, MSE, RMSE, R-squared.
   - **Classification:** Accuracy, Precision, Recall, F1 Score, ROC-AUC.

### 9. **Overfitting and Underfitting:**
   - **Overfitting:** Model learns training data too well, performs poorly on new data.
   - **Underfitting:** Model is too simple, fails to capture underlying patterns.

### 10. **Feature Importance:**
   - **Methods:** Tree-based models provide feature importance scores.
   - **Purpose:** Identifying which features contribute most to model predictions.

### 11. **Neural Networks:**
   - **Architecture:** Input layer, hidden layers, output layer.
   - **Activation Functions:** Sigmoid, ReLU, Tanh.

### 12. **Reinforcement Learning:**
   - **Definition:** Learning by interacting with an environment to achieve a goal.
   - **Algorithms:** Q-Learning, Deep Q Networks (DQN).

### 13. **Natural Language Processing (NLP):**
   - **Tasks:** Text classification, Named Entity Recognition, Sentiment Analysis.
   - **Models:** Word Embeddings (Word2Vec, GloVe), Transformers (BERT, GPT).

### 14. **Clustering:**
   - **Definition:** Grouping similar data points together.
   - **Algorithms:** K-Means, DBSCAN, Hierarchical.

### 15. **Dimensionality Reduction:**
   - **Purpose:** Reducing the number of features while retaining important information.
   - **Techniques:** PCA, t-SNE.

### 16. **Transfer Learning:**
   - **Definition:** Leveraging knowledge learned from one task to improve performance on another.
   - **Applications:** Fine-tuning pre-trained models.

### 17. **Time Series Analysis:**
   - **Components:** Trend, Seasonality, Noise.
   - **Models:** ARIMA, Exponential Smoothing, LSTM.

### 18. **Bias and Fairness:**
   - **Considerations:** Detecting and mitigating biases in models to ensure fair outcomes.
   - **Metrics:** Equal Opportunity, Demographic Parity.
